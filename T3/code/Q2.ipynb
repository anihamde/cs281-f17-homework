{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x1637ea850>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/pytorch/text/blob/master/torchtext/datasets/imdb.py\n",
    "import os, glob, gzip, random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data, datasets, vocab\n",
    "from scipy.stats import norm\n",
    "random.seed(1234)\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load jester dataset, tested on Python 2.7\n",
    "#### Arguments\n",
    "# -load_text: Load text or not. In problem 2, text is unnecessary hence this flag should be\n",
    "#             set to False to save memory. Default True.\n",
    "# -batch_size: batch size. Default 1.\n",
    "# -subsample_rate: Change this to 0.2 in problem 3 and use default 1.0 in problem 2. Default: 1.0\n",
    "# -repeat: Whether to repeat the iterator for multiple epochs. If set to False, \n",
    "#          then .init_epoch() needs to be called before starting next epoch. Default False.\n",
    "# -shuffle: Whether to shuffle examples between epochs.\n",
    "# -ratings_path: The path to user, joke, rating file.\n",
    "# -jokes_path: The path to jokes file \n",
    "# -max_vocab_size: Only the most max_vocab_size frequent words would be kept. We use \n",
    "#                  this to reduce memory footprint and the number of model parameters. Default: 150.\n",
    "# -gpu: Use GPU or not. Default False.\n",
    "#\n",
    "#### Returns\n",
    "# -train_iter: An iterator for training examples. You can call \"for batch in train_iter\"\n",
    "#  to get the training batches. Note that if repeat=False, then\n",
    "#  train_iter.init_epoch() needs to be called before starting next epoch\n",
    "# -val_iter: An iterator for validation examples.\n",
    "# -test_iter: An iterator for test examples.\n",
    "# -text_field (when load_text is True): A field object, text_field.vocab is the vocabulary. \n",
    "#\n",
    "#### Note:\n",
    "# batch.ratings are ratings, can be 1, 2, 3, 4 or 5.\n",
    "# batch.users are user ids, ranging from 1 to 150.\n",
    "# batch.jokes are joke ids, ranging from 1 to 63978.\n",
    "#\n",
    "#### Example 1:\n",
    "# train_iter, val_iter, test_iter, text_field = load_jester(batch_size=100, subsample_rate=1.0, load_text=True)\n",
    "# V = len(text_field.vocab) # vocab size\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_iter.init_epoch()\n",
    "#     for batch in train_iter:\n",
    "#         text = batch.text[0] # x is a tensor of size batch_size x max_len, where max_len\n",
    "#                           # is the maximum joke length in the batch. The other jokes with\n",
    "#                           # length < max_len are padded with text_field.vocab.stoi['<pad>']\n",
    "#         ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "#                                # and we want that to be 0/1/2/3/4.\n",
    "#         users = batch.users-1 \n",
    "#         jokes = batch.jokes-1 \n",
    "#### Example 2 (word id to word str, word str to word id):\n",
    "# word_id = 5\n",
    "# word_str = text_field.vocab.itos[word_id]\n",
    "# word_id = text_field.vocab.stoi[word_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore this, irrelevant to homework\n",
    "class Example(data.Example):\n",
    "    @classmethod\n",
    "    def fromlist(cls, data, fields):\n",
    "        ex = cls()\n",
    "        for (name, field), val in zip(fields, data):\n",
    "            if field is not None:\n",
    "                setattr(ex, name, field.preprocess(val))\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_jester(load_text=True, batch_size=1, subsample_rate=1.0, repeat=False, shuffle=True,\n",
    "        ratings_path='jester_ratings.dat.gz', jokes_path='jester_items.clean.dat.gz', max_vocab_size=150, gpu=False):\n",
    "    DEV = 0 if gpu else -1\n",
    "    assert os.path.exists(jokes_path), \"jokes file %s does not exist!\"%jokes_path\n",
    "    assert os.path.exists(ratings_path), \"ratings file %s does not exist!\"%ratings_path\n",
    "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "    rating_field = data.Field(sequential=False, use_vocab=False)\n",
    "    user_field = data.Field(sequential=False, use_vocab=False)\n",
    "    joke_field = data.Field(sequential=False, use_vocab=False)\n",
    "    if load_text:\n",
    "        fields = [('text', text_field), ('ratings', rating_field), ('users', user_field), ('jokes', joke_field)]\n",
    "    else:\n",
    "        fields = [('ratings', rating_field), ('users', user_field), ('jokes', joke_field)]\n",
    "    jokes_text = {}\n",
    "    joke = -1\n",
    "    all_tokens = []\n",
    "    with gzip.open(jokes_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            l = line.decode('utf-8')\n",
    "            if len(l.strip()) == 0:\n",
    "                continue\n",
    "            if l.strip()[-1] == ':':\n",
    "                joke = int(l.strip().strip(':'))\n",
    "            else:\n",
    "                joke_text = l.strip()\n",
    "                tokens = l.strip().split()\n",
    "                all_tokens.extend(tokens)\n",
    "                jokes_text[joke] = joke_text\n",
    "    counts = Counter(all_tokens)\n",
    "    most_common = counts.most_common(max_vocab_size)\n",
    "    most_common = set([item[0] for item in most_common])\n",
    "\n",
    "\n",
    "    print ('Loading Data, this might take several minutes')\n",
    "    if subsample_rate < 1.0:\n",
    "        print ('Subsampling rate set to %f'%subsample_rate)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    with gzip.open(ratings_path) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if i % 100000 == 0:\n",
    "                print ('%d lines read'%i)\n",
    "            user, joke, rating = l.split()\n",
    "            user = int(user)\n",
    "            joke = int(joke)\n",
    "            rating = int(rating)\n",
    "            if load_text:\n",
    "                assert joke in jokes_text\n",
    "                example = Example.fromlist([' '.join([item for item in jokes_text[joke].split() if item in most_common]), rating, user, joke], fields)\n",
    "            else:\n",
    "                example = Example.fromlist([rating, user, joke], fields)\n",
    "            p = random.random()\n",
    "            q = random.random()\n",
    "            if p < 0.98:\n",
    "                if q < subsample_rate:\n",
    "                    train.append(example)\n",
    "            elif p < 0.99:\n",
    "                val.append(example)\n",
    "            elif p < 1.0:\n",
    "                test.append(example)\n",
    "        train = data.Dataset(train, fields)\n",
    "        val = data.Dataset(val, fields)\n",
    "        test = data.Dataset(test, fields)\n",
    "        train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "            (train, val, test), \n",
    "            batch_size=batch_size, device=DEV, repeat=repeat,\n",
    "            shuffle=shuffle)\n",
    "        train_iter.sort_key = lambda p: len(p.text) if hasattr(p, 'text') else 0\n",
    "        val_iter.sort_key = lambda p: len(p.text) if hasattr(p, 'text') else 0\n",
    "        test_iter.sort_key = lambda p: len(p.text) if hasattr(p, 'text') else 0\n",
    "\n",
    "    print ('Data Loaded')\n",
    "\n",
    "    if load_text:\n",
    "        text_field.build_vocab(train)\n",
    "        return train_iter, val_iter, test_iter, text_field\n",
    "    else:\n",
    "        return train_iter, val_iter, test_iter,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PyTorch function for calcuating log \\phi(x)\n",
    "# example usage: normlogcdf1 = NormLogCDF()((h-b_r)/sigma)\n",
    "class NormLogCDF(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        input_numpy = input.numpy()\n",
    "        output = torch.Tensor(norm.logcdf(input_numpy))\n",
    "        self.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        input_numpy = input.numpy()\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input = grad_input * torch.Tensor(np.exp(norm.logpdf(input_numpy) - norm.logcdf(input_numpy)))\n",
    "        # clip infinities to 1000\n",
    "        grad_input[grad_input==float('inf')] = 1000\n",
    "        # clip -infinities to -1000\n",
    "        grad_input[grad_input==float('-inf')] = -1000\n",
    "        # set nans to 0\n",
    "        grad_input[grad_input!=grad_input] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 2\n",
    "u = torch.nn.Embedding(70000,K)\n",
    "v = torch.nn.Embedding(150,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ind = Variable(torch.LongTensor([ [0] ]))\n",
    "def loglikf(rij,uvdot,sigsq):\n",
    "    return -math.log(1/math.sqrt(2*math.pi*sigsq))-((rij - uvdot)**2)/(2*sigsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.418938533205\n"
     ]
    }
   ],
   "source": [
    "print loglikf(1,2,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PyTorch function for calculating log (\\phi(x) - \\phi(y)) where \\phi is the normal distribution cdf\n",
    "#### Arguments\n",
    "# -x: a PyTorch Variable of size (batch_size).\n",
    "# -y: a PyTorch Variable of size (batch_size). x[i] should be always greater than y[i].\n",
    "#### Returns\n",
    "# log (phi (x) - \\phi(y))\n",
    "def log_difference(x, y):\n",
    "    # calculate by using p1 and p2\n",
    "    logp1 = NormLogCDF()(x)\n",
    "    logp2 = NormLogCDF()(y)\n",
    "    logp = logp1 + torch.log(1 - torch.exp(logp2-logp1))\n",
    "    # calculate by using 1-p1 and 1-p2\n",
    "    log1_p1 = NormLogCDF()(-x)\n",
    "    log1_p2 = NormLogCDF()(-y)\n",
    "    logp_ = log1_p2 + torch.log(1 - torch.exp(log1_p1-log1_p2))\n",
    "    return torch.max(logp, logp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lin_reg_torch(nfeats,nclasses):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module('linear',torch.nn.Linear(nfeats,nclasses))\n",
    "    return model\n",
    "\n",
    "def train(model,loss,optimizer,x,y):\n",
    "    x = Variable(x)\n",
    "    y = Variable(y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    forward_x = model.forward(x)\n",
    "    obj = loss.forward(forward_x,y)\n",
    "\n",
    "    obj.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return obj.data[0]\n",
    "\n",
    "def predict(model, x):\n",
    "    # x = Variable(x)\n",
    "    forward_x = model.forward(x)\n",
    "    return forward_x.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data, this might take several minutes\n",
      "0 lines read\n",
      "100000 lines read\n",
      "200000 lines read\n",
      "300000 lines read\n",
      "400000 lines read\n",
      "500000 lines read\n",
      "600000 lines read\n",
      "700000 lines read\n",
      "800000 lines read\n",
      "900000 lines read\n",
      "1000000 lines read\n",
      "1100000 lines read\n",
      "1200000 lines read\n",
      "1300000 lines read\n",
      "1400000 lines read\n",
      "1500000 lines read\n",
      "1600000 lines read\n",
      "1700000 lines read\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "train2,val2,test2 = load_jester(load_text=False,batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,K):\n",
    "        super(Model,self).__init__()\n",
    "        self.usersfeats = torch.nn.Embedding(70000,K)\n",
    "        self.jokesfeats = torch.nn.Embedding(151,K)\n",
    "    \n",
    "    def forward(self,inds_user,inds_jokes):\n",
    "        innerprod = torch.sum(self.usersfeats(inds_user)*self.jokesfeats(inds_jokes),1)\n",
    "        return innerprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-ed6d807d3c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# batch.rating is a tensor containing actual ratings 1/2/3/4/5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             self.batches = pool(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                 random_shuffler=self.random_shuffler)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m\"\"\"Shuffle and return a new list.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_internal_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/random.pyc\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m         \u001b[0;31m# invariant:  non-selected at [0,n-i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# move non-selected item into vacancy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# K = 2\n",
    "sigsq = 1.0\n",
    "eta = 0.1\n",
    "num_epochs = 3\n",
    "\n",
    "valrmses = []\n",
    "testrmses = []\n",
    "trainrmses = []\n",
    "\n",
    "for K in range(1,11):\n",
    "    val_RMSE = 0\n",
    "    val_cntr = 0\n",
    "    test_RMSE = 0\n",
    "    test_cntr = 0\n",
    "    train_RMSE = 0\n",
    "    train_cntr = 0\n",
    "    \n",
    "    usersfeats = torch.nn.Embedding(70000,K)\n",
    "    jokesfeats = torch.nn.Embedding(150,K) #     model = Model(K)\n",
    "#     jinter = jokesfeats.weight.data.numpy() #     jj = model.jokesfeats\n",
    "#     jj = copy.deepcopy(jinter)\n",
    "    optimizer = torch.optim.SGD([jokesfeats.weight,usersfeats.weight],lr=eta) # model.parameters(),lr=eta)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train2.init_epoch()\n",
    "        for batch in train2:\n",
    "            ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                       # and we want that to be 0/1/2/3/4.\n",
    "            users = batch.users-1 \n",
    "            jokes = batch.jokes-1\n",
    "\n",
    "            inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "            inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "\n",
    "#             innerprod = model.forward(inds_user,inds_jokes) # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(jokesfeats(inds_jokes)*usersfeats(inds_user),1),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "            loss = (sqerror)/(2*sigsq) # *(1/len(innerprod.data.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        print \"trained %s, epoch %s\"%(K,epoch)\n",
    "\n",
    "    for batch in val2:\n",
    "        ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                   # and we want that to be 0/1/2/3/4.\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "        inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "\n",
    "        sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "        loss = (sqerror)/(2*sigsq) \n",
    "        # *(1/len(innerprod.data.numpy())) # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        \n",
    "        predval = torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        val_RMSE += torch.sum((predval - ratings.type(torch.FloatTensor))**2)\n",
    "        val_cntr += len(predval.data.numpy())\n",
    "        \n",
    "#         print val_cntr\n",
    "        \n",
    "    valrmses.append(((val_RMSE.data.numpy()[0])/val_cntr)**(0.5))\n",
    "    print valrmses[-1],val_cntr\n",
    "    \n",
    "    for batch in test2:\n",
    "        ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                   # and we want that to be 0/1/2/3/4.\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "        inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "\n",
    "        sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "        loss = (sqerror)/(2*sigsq) # *(1/len(innerprod.data.numpy()))\n",
    "        # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        \n",
    "        predtest = torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        test_RMSE += torch.sum((predtest - ratings.type(torch.FloatTensor))**2)\n",
    "        test_cntr += len(predtest.data.numpy())\n",
    "        \n",
    "#         print val_cntr\n",
    "        \n",
    "    testrmses.append(((test_RMSE.data.numpy()[0])/test_cntr)**(0.5))\n",
    "    print testrmses[-1],test_cntr\n",
    "    \n",
    "    for batch in train2:\n",
    "        ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                   # and we want that to be 0/1/2/3/4.\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "        inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "\n",
    "        sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "        loss = (sqerror)/(2*sigsq) \n",
    "        # *(1/len(innerprod.data.numpy()))\n",
    "        # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        \n",
    "        predtrain = torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        train_RMSE += torch.sum((predtrain - ratings.type(torch.FloatTensor))**2)\n",
    "        train_cntr += len(predtrain.data.numpy())\n",
    "        \n",
    "#         print train_cntr\n",
    "        \n",
    "    trainrmses.append(((train_RMSE.data.numpy()[0])/train_cntr)**(0.5))\n",
    "    print trainrmses[-1],train_cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print test_RMSE, val_RMSE, train_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-0.4912\n",
       " 0.0631\n",
       "-0.8695\n",
       "-0.0235\n",
       " 0.2256\n",
       "-0.5616\n",
       " 0.8018\n",
       " 0.2404\n",
       " 1.3535\n",
       "-1.0316\n",
       " 0.3283\n",
       "-0.0952\n",
       " 1.8953\n",
       "-0.2617\n",
       "-0.3830\n",
       " 0.6190\n",
       " 0.0557\n",
       "-1.0260\n",
       "-1.2016\n",
       " 0.3934\n",
       " 0.0304\n",
       " 0.5886\n",
       "-0.7764\n",
       "-0.6609\n",
       " 0.7594\n",
       " 0.4348\n",
       "-0.2745\n",
       " 1.6237\n",
       " 0.2187\n",
       " 0.5889\n",
       "-0.3514\n",
       "-0.7929\n",
       " 0.3931\n",
       "-0.1372\n",
       " 0.2976\n",
       "-0.3363\n",
       "-0.3690\n",
       " 0.6460\n",
       " 0.3366\n",
       "-0.6922\n",
       "-0.4670\n",
       "-1.5891\n",
       " 0.9637\n",
       " 2.0889\n",
       "-0.4492\n",
       "-0.5623\n",
       "-0.8382\n",
       "-0.4349\n",
       "-2.2254\n",
       "-0.5203\n",
       " 1.7069\n",
       "-0.2346\n",
       "-0.2412\n",
       " 1.4362\n",
       "-1.8415\n",
       "-1.1993\n",
       " 0.1150\n",
       " 0.7530\n",
       "-1.7088\n",
       " 0.5079\n",
       "-0.3731\n",
       "-0.8246\n",
       " 0.8406\n",
       "-1.1249\n",
       "-0.7094\n",
       " 2.0205\n",
       " 0.4288\n",
       "-0.2266\n",
       " 0.4405\n",
       " 0.1001\n",
       "-1.6828\n",
       "-1.3644\n",
       "-0.0558\n",
       " 0.9146\n",
       " 0.4125\n",
       "-1.6856\n",
       " 1.5639\n",
       "-0.8841\n",
       " 0.5744\n",
       " 1.9769\n",
       " 1.2566\n",
       " 1.5846\n",
       " 0.3157\n",
       "-0.3791\n",
       " 0.8348\n",
       "-1.0050\n",
       " 0.3863\n",
       "-1.4546\n",
       "-0.4096\n",
       " 0.1729\n",
       " 1.5468\n",
       "-1.0196\n",
       " 0.4184\n",
       " 0.7269\n",
       " 1.0911\n",
       " 0.3391\n",
       " 0.4730\n",
       " 0.5163\n",
       " 1.2311\n",
       " 1.1840\n",
       " 0.6907\n",
       " 2.4943\n",
       " 0.0064\n",
       "-0.5533\n",
       "-0.8083\n",
       "-0.6516\n",
       " 0.9552\n",
       "-0.5806\n",
       "-0.8419\n",
       "-0.7018\n",
       "-0.3806\n",
       "-0.2238\n",
       "-0.2506\n",
       "-1.8640\n",
       " 0.8074\n",
       " 0.0291\n",
       "-0.4569\n",
       "-0.2663\n",
       " 1.0749\n",
       " 0.1500\n",
       "-0.0474\n",
       "-1.7623\n",
       " 0.4415\n",
       "-0.1125\n",
       " 0.8628\n",
       " 1.1102\n",
       "-0.0698\n",
       "-0.1481\n",
       "-2.0495\n",
       "-0.8367\n",
       " 0.8870\n",
       " 0.4138\n",
       "-0.5687\n",
       "-0.9596\n",
       " 1.0754\n",
       " 1.1045\n",
       " 1.7180\n",
       " 0.5078\n",
       "-0.4809\n",
       "-1.4110\n",
       " 1.0880\n",
       "-1.5957\n",
       "-1.1839\n",
       "-1.6261\n",
       " 1.0533\n",
       "-0.7722\n",
       " 0.0281\n",
       " 0.4386\n",
       " 0.0128\n",
       "-1.4956\n",
       "[torch.FloatTensor of size 150x1]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokesfeats = torch.nn.Embedding(150,K)\n",
    "jokesfeats.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.9948  0.5036 -0.2942 -0.0985\n",
       "-1.6511  1.5819  0.7101  1.4378\n",
       " 1.7107  1.3247 -1.6435 -1.3916\n",
       " 0.3838  1.1673 -0.6978 -1.9112\n",
       "-0.6337  0.3300 -0.1536 -1.0553\n",
       " 0.0956  1.2218 -1.4849 -1.3780\n",
       "-1.2208  1.5846  0.8946 -0.8819\n",
       "-2.5031 -0.7906 -0.2690  0.1350\n",
       " 0.2481  1.3830  0.8811 -1.1945\n",
       "-1.0297  1.1959  0.3188  0.2053\n",
       "[torch.FloatTensor of size 10x4]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersfeats(inds_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.4858  1.3756 -0.6989 -0.9970\n",
       "-1.2640  1.4753 -0.3965 -0.7435\n",
       "-1.3660  1.5983 -0.5289 -0.8458\n",
       "-1.4170  1.5523 -0.4898 -0.9015\n",
       "-1.3823  1.3664 -0.4989 -0.7994\n",
       "-1.2505  1.1387 -0.5301 -0.8422\n",
       "-0.9590  1.0148 -0.3333 -0.8742\n",
       "-1.2559  1.1467 -0.4537 -0.7781\n",
       "-1.3731  1.4593 -0.5648 -0.7981\n",
       "-1.3723  1.3030 -0.4903 -0.9104\n",
       "[torch.FloatTensor of size 10x4]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokesfeats(inds_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1496e-04\n",
       " 1.7510e-02\n",
       "-4.6969e-03\n",
       " 3.8812e-03\n",
       " 4.0440e-03\n",
       " 9.9134e-03\n",
       " 2.5107e-02\n",
       " 3.6534e-02\n",
       " 2.3300e-03\n",
       "-2.5831e-02\n",
       " 9.5901e-02\n",
       "-1.1241e-02\n",
       " 5.7685e-02\n",
       " 3.1562e-02\n",
       "-5.5874e-03\n",
       " 1.0018e-02\n",
       " 8.2120e-04\n",
       " 2.1525e-02\n",
       "-2.5186e-02\n",
       "-1.5395e-02\n",
       " 1.7725e-02\n",
       "-1.2393e-02\n",
       "-6.4497e-03\n",
       "-6.1645e-03\n",
       "-7.1428e-03\n",
       " 7.5400e-04\n",
       "-1.5026e-03\n",
       " 1.2218e-02\n",
       "-1.7118e-03\n",
       " 1.9977e-02\n",
       "-5.8901e-04\n",
       " 6.3182e-03\n",
       " 9.3312e-03\n",
       "-1.6528e-02\n",
       " 3.0061e-03\n",
       " 1.4435e-02\n",
       "-3.5218e-03\n",
       "-5.7108e-02\n",
       " 4.7205e-02\n",
       "-2.7426e-02\n",
       "-8.7882e-04\n",
       "-1.4990e-02\n",
       "-2.8393e-02\n",
       "-7.2167e-03\n",
       " 1.5428e-02\n",
       " 2.8546e-02\n",
       " 6.5519e-04\n",
       " 1.7365e-02\n",
       " 2.6103e-02\n",
       " 7.8218e-03\n",
       "-9.7666e-02\n",
       "-1.5205e-02\n",
       "-4.8704e-03\n",
       " 2.9948e-02\n",
       "-3.4937e-03\n",
       "-1.6972e-02\n",
       "-4.2239e-03\n",
       " 4.4436e-02\n",
       "-8.4850e-03\n",
       "-5.6003e-03\n",
       "-8.0138e-03\n",
       "-2.2477e-02\n",
       " 3.3048e-02\n",
       "-1.3476e-02\n",
       " 7.9103e-03\n",
       "-1.2879e-02\n",
       " 8.9745e-03\n",
       "-2.8447e-02\n",
       "-5.5703e-03\n",
       "-2.1857e-03\n",
       " 6.0662e-04\n",
       "-1.1088e-04\n",
       "-2.5615e-02\n",
       "-2.9565e-03\n",
       "-9.7761e-04\n",
       "-6.4519e-02\n",
       " 3.0295e-02\n",
       " 2.7504e-02\n",
       " 3.6038e-03\n",
       "-6.9601e-04\n",
       "-2.4567e-03\n",
       "-8.8986e-05\n",
       "-7.2501e-02\n",
       "-3.4298e-02\n",
       " 2.3818e-02\n",
       " 2.0717e-02\n",
       "-2.6628e-02\n",
       " 7.6177e-05\n",
       "-7.1923e-05\n",
       " 7.4347e-04\n",
       "-3.3528e-03\n",
       " 3.4831e-02\n",
       " 5.9881e-03\n",
       "-2.6776e-02\n",
       "-2.2512e-03\n",
       " 6.3610e-03\n",
       " 5.4224e-03\n",
       " 1.0410e-02\n",
       "-1.9252e-02\n",
       "-6.1923e-03\n",
       " 4.4287e-04\n",
       "-2.6509e-03\n",
       "-6.3008e-05\n",
       "-1.9438e-02\n",
       " 3.1029e-03\n",
       " 6.4592e-03\n",
       " 8.7757e-03\n",
       "-3.8600e-02\n",
       " 3.8377e-03\n",
       "-1.9498e-03\n",
       " 1.8146e-03\n",
       " 7.7809e-03\n",
       " 8.2095e-05\n",
       " 7.1340e-04\n",
       "-6.7783e-02\n",
       "-3.5648e-02\n",
       " 3.3362e-03\n",
       "-2.3356e-02\n",
       " 9.0342e-03\n",
       "-5.0626e-04\n",
       "-1.2483e-02\n",
       " 2.2352e-03\n",
       "-1.0612e-03\n",
       "-3.2530e-03\n",
       " 1.6682e-02\n",
       "-2.7860e-04\n",
       " 3.6456e-02\n",
       "-2.2386e-02\n",
       "-2.4282e-05\n",
       " 2.2322e-03\n",
       "-8.2607e-04\n",
       "-9.5665e-02\n",
       " 4.8388e-02\n",
       " 7.1300e-03\n",
       "-3.6178e-02\n",
       "-2.3910e-02\n",
       "-2.7621e-03\n",
       " 1.4460e-02\n",
       " 5.4147e-03\n",
       "-2.6732e-02\n",
       " 2.6485e-02\n",
       " 1.6167e-05\n",
       "-1.7117e-05\n",
       "-5.4675e-02\n",
       " 3.4951e-03\n",
       " 7.6769e-03\n",
       " 9.9790e-03\n",
       "-2.5890e-02\n",
       " 7.4822e-03\n",
       "-3.3558e-03\n",
       "-4.9085e-03\n",
       " 9.7105e-05\n",
       "-4.3055e-03\n",
       "-1.3910e-02\n",
       "-1.8201e-02\n",
       "-4.5794e-02\n",
       "-2.6770e-03\n",
       " 6.4099e-02\n",
       "-1.1978e-02\n",
       "-8.0015e-03\n",
       " 8.4043e-03\n",
       " 8.1369e-03\n",
       " 1.7601e-02\n",
       "-2.2733e-04\n",
       " 9.0303e-03\n",
       " 8.5924e-03\n",
       " 5.4304e-03\n",
       "-6.1136e-04\n",
       "-1.6386e-02\n",
       "-1.2485e-02\n",
       "-1.4169e-04\n",
       " 2.3794e-02\n",
       " 2.9443e-02\n",
       " 3.4623e-02\n",
       " 1.5118e-03\n",
       "-1.8873e-03\n",
       " 1.0318e-03\n",
       "-2.2575e-02\n",
       " 5.3176e-04\n",
       "-4.7710e-03\n",
       "-2.0347e-04\n",
       "-2.8075e-02\n",
       " 1.3629e-03\n",
       " 3.0558e-03\n",
       " 2.9326e-04\n",
       "-2.5914e-02\n",
       "-6.6372e-02\n",
       "-3.2578e-02\n",
       "-2.2277e-02\n",
       "-4.6952e-03\n",
       "-2.8341e-04\n",
       "-5.6642e-02\n",
       "-3.2448e-02\n",
       " 1.3499e-02\n",
       "-1.8368e-02\n",
       " 1.1797e-02\n",
       "-2.0049e-02\n",
       "-1.6610e-03\n",
       "-2.6001e-02\n",
       " 1.0708e-02\n",
       "-5.6901e-04\n",
       " 8.8051e-04\n",
       " 3.6025e-04\n",
       " 3.9067e-02\n",
       " 3.2173e-02\n",
       " 5.3148e-02\n",
       "-3.9258e-02\n",
       " 3.2178e-02\n",
       " 2.0986e-02\n",
       " 1.5130e-03\n",
       " 2.8883e-03\n",
       "-2.5197e-03\n",
       "-1.1036e-02\n",
       "-2.2899e-02\n",
       " 3.4723e-02\n",
       "-7.4474e-04\n",
       " 4.2155e-04\n",
       " 8.3368e-04\n",
       "-8.3109e-04\n",
       "-7.4700e-03\n",
       "-1.8324e-03\n",
       " 5.5856e-02\n",
       "-4.2923e-02\n",
       "-7.4607e-03\n",
       "-4.4386e-04\n",
       " 6.8521e-05\n",
       " 9.4944e-02\n",
       " 7.3271e-03\n",
       " 5.1411e-03\n",
       "-6.7674e-04\n",
       " 1.1490e-03\n",
       " 4.2614e-03\n",
       "-1.8562e-02\n",
       " 5.0790e-03\n",
       "-2.0363e-02\n",
       " 3.1961e-02\n",
       "-2.7192e-02\n",
       " 1.0597e-04\n",
       " 6.1085e-05\n",
       " 2.3665e-06\n",
       " 1.6235e-02\n",
       " 2.7503e-02\n",
       " 5.7545e-02\n",
       " 7.6567e-03\n",
       "-1.8880e-02\n",
       "-1.4413e-03\n",
       " 2.8990e-03\n",
       " 1.0092e-02\n",
       "-4.1381e-02\n",
       " 6.7532e-02\n",
       "-3.8257e-04\n",
       " 3.6038e-03\n",
       "-1.2922e-04\n",
       " 3.3108e-02\n",
       "-1.0865e-02\n",
       " 2.6337e-02\n",
       " 6.6358e-03\n",
       " 2.2553e-04\n",
       "-7.6039e-02\n",
       " 4.0078e-04\n",
       " 1.7679e-03\n",
       "-9.7373e-03\n",
       "-1.7738e-02\n",
       "-1.0251e-02\n",
       " 5.7358e-07\n",
       " 2.7546e-03\n",
       "-1.0813e-02\n",
       " 1.9755e-02\n",
       "-5.9910e-04\n",
       " 9.3942e-02\n",
       " 1.0401e-02\n",
       "-2.8290e-04\n",
       " 6.2361e-04\n",
       "-1.4007e-05\n",
       " 2.1006e-02\n",
       "-2.6683e-04\n",
       "-2.9869e-02\n",
       " 9.7364e-03\n",
       " 8.1353e-03\n",
       "-4.6238e-02\n",
       " 1.6054e-03\n",
       " 1.9779e-02\n",
       " 2.9551e-02\n",
       "-2.0596e-02\n",
       " 6.0985e-02\n",
       " 2.8224e-02\n",
       " 2.2591e-03\n",
       " 5.3605e-04\n",
       " 1.2993e-02\n",
       "-6.9226e-03\n",
       " 5.1059e-03\n",
       " 5.1915e-02\n",
       " 4.8679e-03\n",
       " 4.5075e-02\n",
       " 2.3059e-02\n",
       " 7.4083e-03\n",
       " 5.0069e-03\n",
       " 3.5323e-03\n",
       "-2.0223e-03\n",
       "-1.8548e-03\n",
       " 8.2045e-04\n",
       "-1.4281e-02\n",
       " 2.3187e-03\n",
       " 3.6637e-03\n",
       "-1.1599e-04\n",
       " 6.3887e-04\n",
       "-2.2615e-02\n",
       " 9.7326e-03\n",
       " 5.0957e-04\n",
       " 2.7507e-04\n",
       "-1.3318e-04\n",
       "-4.9617e-03\n",
       " 3.9053e-03\n",
       " 8.9880e-03\n",
       " 3.8494e-02\n",
       " 3.4702e-03\n",
       "-8.2949e-03\n",
       "-9.8974e-04\n",
       "-5.5019e-03\n",
       "-1.0077e-03\n",
       " 2.7308e-02\n",
       " 5.7439e-02\n",
       " 1.6264e-02\n",
       "-1.0580e-02\n",
       "-2.1253e-04\n",
       "-1.4867e-02\n",
       "-1.9608e-02\n",
       " 1.0441e-02\n",
       "-5.9659e-02\n",
       " 5.2881e-02\n",
       "-1.0349e-02\n",
       " 1.8655e-02\n",
       "-3.8176e-03\n",
       "-2.4377e-03\n",
       " 3.6601e-03\n",
       " 1.8663e-03\n",
       "-1.6251e-02\n",
       " 3.6893e-04\n",
       "-5.9104e-02\n",
       "-2.9321e-03\n",
       "-2.1387e-02\n",
       "-4.4702e-03\n",
       "-1.5516e-03\n",
       "-2.5138e-03\n",
       "-1.1027e-03\n",
       " 2.2243e-02\n",
       " 4.4436e-03\n",
       " 1.2987e-02\n",
       "-1.5279e-03\n",
       " 2.5387e-02\n",
       " 2.8959e-03\n",
       " 7.7975e-03\n",
       " 5.8047e-04\n",
       "-9.0265e-04\n",
       " 1.8925e-03\n",
       " 3.8427e-02\n",
       "-5.5684e-04\n",
       " 2.4747e-03\n",
       "-2.3609e-03\n",
       " 2.8010e-02\n",
       " 1.8629e-02\n",
       " 8.6741e-03\n",
       " 6.8468e-03\n",
       " 2.9483e-02\n",
       " 3.0734e-03\n",
       " 1.4506e-02\n",
       "-1.0247e-02\n",
       " 8.9415e-03\n",
       " 5.2889e-03\n",
       "-1.2704e-02\n",
       "-5.9090e-05\n",
       " 8.5595e-04\n",
       "-1.7554e-02\n",
       "-6.2954e-04\n",
       " 3.8981e-04\n",
       "-4.4082e-02\n",
       " 2.7611e-03\n",
       " 6.8915e-04\n",
       " 5.5673e-03\n",
       "-3.9848e-03\n",
       "-1.5072e-03\n",
       "-2.2868e-02\n",
       " 4.7709e-02\n",
       " 3.5656e-02\n",
       " 8.0853e-03\n",
       " 1.7244e-02\n",
       " 1.4495e-02\n",
       " 1.3118e-02\n",
       "-2.2374e-02\n",
       " 4.2288e-02\n",
       "-1.5719e-02\n",
       "-2.3956e-02\n",
       "-3.1705e-02\n",
       " 1.6121e-04\n",
       " 3.0780e-02\n",
       "-1.3233e-02\n",
       " 3.4473e-02\n",
       " 5.5980e-03\n",
       " 9.6579e-04\n",
       "-4.6334e-04\n",
       " 1.7090e-02\n",
       " 1.3214e-02\n",
       "-1.6185e-02\n",
       "-9.8519e-03\n",
       "-3.0854e-04\n",
       "-4.6369e-04\n",
       " 1.0652e-03\n",
       " 2.2755e-03\n",
       " 2.1004e-03\n",
       " 1.9610e-02\n",
       "-6.5024e-04\n",
       "-6.8077e-02\n",
       "-2.4977e-02\n",
       "-1.5686e-03\n",
       "-7.1736e-03\n",
       "-2.2521e-02\n",
       "-1.2637e-04\n",
       "-4.4261e-04\n",
       " 9.5051e-03\n",
       "-7.0332e-04\n",
       " 4.3367e-03\n",
       " 3.8511e-03\n",
       " 6.6245e-03\n",
       "-4.6777e-02\n",
       " 2.9227e-03\n",
       "-8.6245e-03\n",
       "-1.3458e-02\n",
       "-6.3758e-03\n",
       " 9.5757e-03\n",
       "-2.2801e-05\n",
       "-8.7793e-06\n",
       " 1.2765e-02\n",
       " 8.5038e-02\n",
       "-1.6083e-02\n",
       "-5.7391e-02\n",
       " 1.4332e-02\n",
       "-1.3207e-02\n",
       " 8.3537e-02\n",
       " 4.1370e-03\n",
       " 9.5655e-04\n",
       " 1.1444e-03\n",
       " 5.8127e-03\n",
       "-7.8234e-03\n",
       " 1.5179e-02\n",
       " 2.1782e-02\n",
       " 7.4210e-03\n",
       " 2.8141e-02\n",
       " 3.4338e-02\n",
       "-1.0004e-02\n",
       " 1.8549e-02\n",
       "-1.2735e-03\n",
       "-2.3051e-02\n",
       "-1.0752e-02\n",
       " 8.4697e-03\n",
       " 3.3832e-03\n",
       "-3.2344e-03\n",
       "-2.5416e-02\n",
       "-7.4741e-03\n",
       " 3.1300e-02\n",
       " 1.5532e-03\n",
       "-1.7824e-03\n",
       " 7.1692e-04\n",
       " 2.9400e-03\n",
       " 9.2189e-03\n",
       " 3.0102e-03\n",
       " 3.4052e-03\n",
       " 1.4637e-03\n",
       "-1.3070e-02\n",
       "-2.9780e-03\n",
       "-1.3155e-02\n",
       "-4.1507e-03\n",
       "-1.8926e-02\n",
       "-1.3682e-02\n",
       "-2.1472e-02\n",
       " 3.5573e-03\n",
       "-1.6404e-03\n",
       " 9.9883e-04\n",
       "-2.4127e-03\n",
       "-5.6625e-03\n",
       " 4.0098e-03\n",
       "-4.4076e-03\n",
       " 4.8618e-03\n",
       "-1.3954e-02\n",
       "-2.8501e-04\n",
       "-1.7227e-02\n",
       " 3.1297e-02\n",
       " 1.7333e-03\n",
       "-3.9324e-02\n",
       "-1.7080e-02\n",
       "-1.3833e-02\n",
       " 1.0190e-02\n",
       " 2.2036e-02\n",
       " 3.1076e-02\n",
       "-4.8383e-01\n",
       "-5.5698e-02\n",
       "-7.5644e-02\n",
       "-5.6229e-02\n",
       "-3.1492e-03\n",
       "-4.1601e-04\n",
       "-8.7864e-03\n",
       "-1.3168e-02\n",
       " 2.7414e-03\n",
       " 1.9511e-02\n",
       "-1.8384e-03\n",
       "-1.5264e-04\n",
       " 1.8839e-02\n",
       " 1.0388e-02\n",
       "-1.7936e-01\n",
       "-4.7851e-03\n",
       " 3.4187e-03\n",
       " 5.3554e-03\n",
       " 2.0291e-02\n",
       " 1.0992e-02\n",
       " 1.7707e-03\n",
       " 4.7247e-03\n",
       " 5.2203e-03\n",
       "-2.1505e-02\n",
       "-2.3810e-03\n",
       "-4.7460e-03\n",
       " 3.2583e-03\n",
       "-8.1818e-03\n",
       "-3.7906e-03\n",
       " 6.3469e-03\n",
       " 2.2261e-02\n",
       " 2.3213e-02\n",
       " 1.5584e-03\n",
       " 3.6783e-03\n",
       "-2.6300e-04\n",
       " 4.1206e-05\n",
       "-2.2229e-05\n",
       " 4.2687e-03\n",
       "-4.4667e-04\n",
       "-4.3467e-02\n",
       " 3.4655e-04\n",
       "-4.8996e-02\n",
       "-1.5433e-02\n",
       " 2.1105e-02\n",
       "-4.0877e-04\n",
       "-3.7797e-02\n",
       "-6.4326e-04\n",
       " 1.6872e-03\n",
       "-3.8491e-03\n",
       "-4.4706e-03\n",
       "-5.6504e-03\n",
       " 8.1351e-05\n",
       "-3.6970e-03\n",
       "-7.1997e-04\n",
       "-1.6688e-02\n",
       " 3.5174e-05\n",
       "-6.7711e-04\n",
       "-1.7684e-02\n",
       "-8.5444e-04\n",
       " 8.9300e-03\n",
       " 4.2680e-02\n",
       " 2.6380e-04\n",
       " 1.7836e-03\n",
       " 3.5504e-04\n",
       " 2.5600e-02\n",
       "-1.3745e-03\n",
       " 7.1288e-03\n",
       "-8.6390e-03\n",
       "-1.9429e-03\n",
       "-1.8607e-03\n",
       "-2.8217e-02\n",
       " 1.1863e-02\n",
       " 7.3204e-03\n",
       " 7.0708e-02\n",
       " 5.0710e-02\n",
       " 5.5184e-03\n",
       " 1.0166e-02\n",
       " 1.6596e-02\n",
       "-2.3265e-04\n",
       " 5.1152e-03\n",
       " 4.5248e-03\n",
       " 1.2316e-01\n",
       " 8.0203e-03\n",
       "-7.1613e-03\n",
       " 2.0952e-02\n",
       " 9.6562e-02\n",
       " 3.0178e-03\n",
       "-1.3744e-02\n",
       " 1.9456e-04\n",
       "-8.3467e-03\n",
       "-1.5844e-02\n",
       "-2.3119e-03\n",
       "-2.1920e-02\n",
       " 2.7254e-02\n",
       "-2.6453e-03\n",
       " 7.9373e-03\n",
       " 7.3896e-04\n",
       "-3.7202e-04\n",
       " 2.9903e-02\n",
       "-2.2185e-03\n",
       "-5.2530e-02\n",
       "-1.7465e-02\n",
       " 8.8048e-03\n",
       " 1.3570e-03\n",
       " 5.1068e-04\n",
       " 4.3805e-03\n",
       "-1.5321e-02\n",
       " 6.0797e-02\n",
       " 2.8857e-03\n",
       "-1.0913e-02\n",
       " 1.2022e-02\n",
       "-4.5433e-02\n",
       " 1.2125e-02\n",
       "-2.1321e-02\n",
       " 1.9835e-03\n",
       " 1.2108e-02\n",
       "-1.5306e-03\n",
       "-4.0808e-03\n",
       " 3.2415e-03\n",
       "-1.0765e-03\n",
       " 7.5631e-02\n",
       " 2.0516e-02\n",
       " 3.6558e-03\n",
       "-5.7042e-03\n",
       " 6.6007e-03\n",
       "-8.8164e-03\n",
       " 1.3394e-02\n",
       " 1.9649e-02\n",
       " 3.5393e-02\n",
       "[torch.FloatTensor of size 622]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.1101 -0.6508 -0.8809 -0.4251  0.1054  1.5055 -2.4429 -1.0579 -1.1910 -0.3968\n",
       "-3.4318  0.6052  0.3852  1.4339  0.4811  1.0113 -1.6192 -0.7617 -0.5112  1.2556\n",
       "[torch.FloatTensor of size 2x10]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersfeats(Variable(torch.LongTensor([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.3801 -0.3368  0.0419  0.3121  0.4789  0.3931 -0.9177  1.0933  2.1546  0.1187\n",
       "-0.4991  0.9449 -0.2203  0.3468 -1.1888  0.4214 -1.2120 -0.7144 -0.1448  0.8207\n",
       "[torch.FloatTensor of size 2x10]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokesfeats(Variable(torch.LongTensor([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6962\n",
       " 6.1626\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(usersfeats(Variable(torch.LongTensor([0,1])))*jokesfeats(Variable(torch.LongTensor([0,1]))),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9aa10c179989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# batch.rating is a tensor containing actual ratings 1/2/3/4/5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             self.batches = pool(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                 random_shuffler=self.random_shuffler)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext-0.2.0b0-py2.7.egg/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# K = 2\n",
    "sigsq = 1.0\n",
    "eta = 0.1\n",
    "num_epochs = 2\n",
    "\n",
    "valrmsesf = []\n",
    "testrmsesf = []\n",
    "trainrmsesf = []\n",
    "\n",
    "for K in range(2,3):\n",
    "    val_RMSE = 0\n",
    "    val_cntr = 0\n",
    "    test_RMSE = 0\n",
    "    test_cntr = 0\n",
    "    train_RMSE = 0\n",
    "    train_cntr = 0\n",
    "    \n",
    "    usersfeats = torch.nn.Embedding(70000,K)\n",
    "    jokesfeats = torch.nn.Embedding(150,K) #     model = Model(K)\n",
    "    a = torch.nn.Embedding(70000,1)\n",
    "    b = torch.nn.Embedding(150,1)\n",
    "    g = torch.nn.Embedding(1,1)\n",
    "#     g = Variable(torch.from_numpy(np.zeros(1)), requires_grad=True) #Variable(torch.from_numpy(np.array(1.)), requires_grad=True) #Variable(torch.Tensor([2])) # torch.nn.Embedding(1,1)\n",
    "#     g = g.type(torch.FloatTensor)\n",
    "    jinter = jokesfeats.weight.data.numpy() #     jj = model.jokesfeats\n",
    "    jj = copy.deepcopy(jinter)\n",
    "    optimizer = torch.optim.SGD([usersfeats.weight,jokesfeats.weight,a.weight,b.weight,g.weight],lr=eta) # model.parameters(),lr=eta)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train2.init_epoch()\n",
    "        for batch in train2:\n",
    "            ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                       # and we want that to be 0/1/2/3/4.\n",
    "            users = batch.users-1 \n",
    "            jokes = batch.jokes-1\n",
    "\n",
    "            inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "            inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "            jsxz = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "            \n",
    "            jsxz.data = torch.LongTensor([0]*len(inds_user))\n",
    "\n",
    "#             innerprod = model.forward(inds_user,inds_jokes) # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)+a(inds_user).squeeze()+b(inds_jokes).squeeze()+g(jsxz).squeeze(),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "            loss = (sqerror)/(2*sigsq) # *(1/len(innerprod.data.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        print \"trained %s, epoch %s\"%(K,epoch)\n",
    "\n",
    "    for batch in val2:\n",
    "        ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                   # and we want that to be 0/1/2/3/4.\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "        inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "        jsxz = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "            \n",
    "        jsxz.data = torch.LongTensor([0]*len(inds_user))\n",
    "\n",
    "#         sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes)+a(inds_user)+b(inds_jokes),2))+g.expand(len(inds_user))) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "#         loss = (sqerror)/(2*sigsq) \n",
    "        # *(1/len(innerprod.data.numpy())) # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        \n",
    "        predval = torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)+a(inds_user).squeeze()+b(inds_jokes).squeeze()+g(jsxz).squeeze()\n",
    "        val_RMSE += torch.sum((predval - ratings.type(torch.FloatTensor))**2)\n",
    "        val_cntr += len(predval.data.numpy())\n",
    "        \n",
    "#         print val_cntr\n",
    "        \n",
    "    valrmsesf.append(((val_RMSE.data.numpy()[0])/val_cntr)**(0.5))\n",
    "    print valrmsesf[-1]\n",
    "    \n",
    "    for batch in test2:\n",
    "        ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                   # and we want that to be 0/1/2/3/4.\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "        inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "        jsxz = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "            \n",
    "        jsxz.data = torch.LongTensor([0]*len(inds_user))\n",
    "        \n",
    "#         sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes)+a(inds_user)+b(inds_jokes),1)+g.expand(len(inds_user)),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "#         loss = (sqerror)/(2*sigsq) # *(1/len(innerprod.data.numpy()))\n",
    "        # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        \n",
    "        predtest = torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)+a(inds_user).squeeze()+b(inds_jokes).squeeze()+g(jsxz).squeeze()\n",
    "        test_RMSE += torch.sum((predtest - ratings.type(torch.FloatTensor))**2)\n",
    "        test_cntr += len(predtest.data.numpy())\n",
    "        \n",
    "#         print val_cntr\n",
    "        \n",
    "    testrmsesf.append(((test_RMSE.data.numpy()[0])/test_cntr)**(0.5))\n",
    "    print testrmsesf[-1]\n",
    "    \n",
    "    for batch in train2:\n",
    "        ratings = batch.ratings-1 # batch.rating is a tensor containing actual ratings 1/2/3/4/5,\n",
    "                                   # and we want that to be 0/1/2/3/4.\n",
    "        users = batch.users-1 \n",
    "        jokes = batch.jokes-1\n",
    "\n",
    "        inds_user = Variable(torch.LongTensor(users.data.numpy()))\n",
    "        inds_jokes = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "        jsxz = Variable(torch.LongTensor(jokes.data.numpy()))\n",
    "            \n",
    "        jsxz.data = torch.LongTensor([0]*len(inds_user))\n",
    "\n",
    "#         sqerror = torch.mean(torch.pow(ratings.type(torch.FloatTensor)-torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes)+a(inds_user)+b(inds_jokes),1)+g.expand(len(inds_user)),2)) # model.forward(inds_user,inds_jokes),2)\n",
    "            \n",
    "#         loss = (sqerror)/(2*sigsq) \n",
    "        # *(1/len(innerprod.data.numpy()))\n",
    "        # torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)\n",
    "        \n",
    "        predtrain = torch.sum(usersfeats(inds_user)*jokesfeats(inds_jokes),1)+a(inds_user).squeeze()+b(inds_jokes).squeeze()+g(jsxz).squeeze()\n",
    "        train_RMSE += torch.sum((predtrain - ratings.type(torch.FloatTensor))**2)\n",
    "        train_cntr += len(predtrain.data.numpy())\n",
    "        \n",
    "#         print train_cntr\n",
    "        \n",
    "    trainrmsesf.append(((train_RMSE.data.numpy()[0])/train_cntr)**(0.5))\n",
    "    print trainrmsesf[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 2.3577\n",
       "[torch.FloatTensor of size 1x1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-3.8690e-02\n",
       "-5.2234e-01\n",
       " 1.1857e+00\n",
       "     ⋮      \n",
       "-4.9123e-01\n",
       " 9.4260e-01\n",
       "-1.0283e+00\n",
       "[torch.FloatTensor of size 70000x1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 1.2927\n",
       " 0.3630\n",
       "-1.3614\n",
       "-0.7207\n",
       " 1.7289\n",
       "-0.3283\n",
       "-0.2725\n",
       "-0.0153\n",
       "-1.6508\n",
       " 0.0394\n",
       "-0.9581\n",
       " 0.4125\n",
       "-0.4558\n",
       "-1.4019\n",
       " 0.4120\n",
       "-0.9003\n",
       "-1.1523\n",
       "-0.6141\n",
       "-0.8906\n",
       " 1.2454\n",
       "-1.2888\n",
       "-0.0946\n",
       " 0.5821\n",
       "-2.0305\n",
       " 0.7308\n",
       "-1.5659\n",
       " 1.0679\n",
       "-2.0993\n",
       "-0.3205\n",
       " 0.6242\n",
       "-0.7717\n",
       " 0.6388\n",
       " 0.5793\n",
       " 0.3892\n",
       "-0.0944\n",
       " 0.0933\n",
       "-0.9935\n",
       " 2.0858\n",
       "-0.3333\n",
       " 1.5665\n",
       "-1.5787\n",
       "-0.1048\n",
       "-0.6989\n",
       " 0.3065\n",
       "-0.7339\n",
       "-1.4539\n",
       "-0.7523\n",
       " 0.3730\n",
       " 0.3767\n",
       "-1.4900\n",
       "-1.2660\n",
       "-2.7776\n",
       " 2.0002\n",
       " 0.5168\n",
       " 1.5059\n",
       "-0.9852\n",
       "-0.1556\n",
       " 0.6789\n",
       " 0.0851\n",
       "-0.6038\n",
       "-0.0509\n",
       " 1.6412\n",
       "-0.2190\n",
       "-1.7538\n",
       " 0.1660\n",
       "-0.5062\n",
       "-0.4556\n",
       " 0.9620\n",
       " 0.0897\n",
       "-1.0589\n",
       "-0.0060\n",
       " 1.0297\n",
       " 0.2579\n",
       "-1.1050\n",
       "-0.8882\n",
       " 0.7862\n",
       " 0.2309\n",
       "-0.5020\n",
       " 0.3128\n",
       " 0.8548\n",
       "-1.7883\n",
       "-0.7496\n",
       "-1.8927\n",
       " 1.6628\n",
       " 1.1660\n",
       " 1.6673\n",
       " 0.3202\n",
       " 1.7129\n",
       "-0.3275\n",
       "-0.1701\n",
       "-2.2393\n",
       "-0.2929\n",
       "-0.9970\n",
       " 0.6260\n",
       "-0.4084\n",
       "-0.6218\n",
       "-0.1352\n",
       "-0.2476\n",
       "-0.5536\n",
       " 0.4067\n",
       "-0.9964\n",
       " 0.9301\n",
       "-0.6633\n",
       "-0.0039\n",
       " 0.0266\n",
       " 0.7100\n",
       " 0.8097\n",
       "-0.3461\n",
       "-1.0909\n",
       "-0.9569\n",
       " 0.6368\n",
       "-0.8489\n",
       " 0.0096\n",
       " 0.5415\n",
       " 0.8711\n",
       " 1.1677\n",
       "-0.9336\n",
       " 0.1388\n",
       "-0.9748\n",
       " 1.1559\n",
       " 0.5268\n",
       " 0.5369\n",
       "-0.2305\n",
       "-0.5168\n",
       " 1.6346\n",
       " 0.7556\n",
       " 0.7379\n",
       "-0.8139\n",
       " 1.0028\n",
       " 0.1012\n",
       "-0.8166\n",
       " 2.8915\n",
       "-1.3170\n",
       " 0.5748\n",
       "-0.3137\n",
       "-1.2514\n",
       "-2.0099\n",
       " 0.8534\n",
       "-0.2778\n",
       " 0.6535\n",
       " 1.7410\n",
       "-0.9111\n",
       " 0.0771\n",
       "-1.1021\n",
       "-0.0964\n",
       " 1.4880\n",
       " 1.2286\n",
       " 1.1386\n",
       " 0.6170\n",
       "-0.6315\n",
       "[torch.FloatTensor of size 150x1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b.weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.89153934], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight.data.numpy()[np.argmax(b.weight.data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(b.weight.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.77755117], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight.data.numpy()[np.argmin(b.weight.data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
